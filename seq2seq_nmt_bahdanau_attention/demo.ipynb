{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq NMT with Bahdanau Attention (English → French)\n",
        "\n",
        "This notebook demonstrates a sequence-to-sequence (Seq2Seq) neural machine translation model enhanced with **Bahdanau-style attention**. Unlike Luong attention (which is post-RNN), Bahdanau computes attention **before** generating the decoder hidden state, using an *alignment model*.\n",
        "\n",
        "We evaluate two alignment strategies:\n",
        "- **Concat (Original Bahdanau)**\n",
        "- **Additive (Optimized MLP variant)**\n",
        "\n",
        "\n",
        "We use a subset of the **English-French** dataset from [Tatoeba (ManyThings.org)](https://www.manythings.org/anki/).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6IwYhOuprnUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports and Setup"
      ],
      "metadata": {
        "id": "GQPbql7wrzlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from dataloader_generator import (\n",
        "    normalizeString,\n",
        "    prepareData,\n",
        "    DatasetEngFra,\n",
        "    collate_batch,\n",
        "    PAD_token,\n",
        "    SOS_token,\n",
        "    EOS_token\n",
        ")\n",
        "\n",
        "from models import EncoderWithBahdanauAttention, DecoderWithBahdanauAttention\n",
        "from utils import (\n",
        "    train_bahdanau_attention,\n",
        "    translate_with_attention,\n",
        "    plot_attention\n",
        ")\n",
        "\n",
        "# Set device and random seed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed = 13\n",
        "torch.manual_seed(seed)\n"
      ],
      "metadata": {
        "id": "hygdE0JHrvlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Hyperparameters & Dataset\n",
        "\n",
        "## Dataset and Hyperparameters\n",
        "\n",
        "- Based on the [Tatoeba English–French dataset](https://www.manythings.org/anki/)\n",
        "- Sentence pairs truncated to a max length of 10 tokens\n",
        "- Preprocessing: Unicode normalization, tokenization, lowercasing\n",
        "\n",
        "We will train on:\n",
        "- **20 epochs**\n",
        "- **Hidden size**: 256\n",
        "- **Batch size**: 32\n",
        "- **Bahdanau alignment modes**: `[\"concat\", \"additive\"]`\n"
      ],
      "metadata": {
        "id": "ANoxuMuCr8ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "num_epochs = 20\n",
        "hidden_size = 256\n",
        "alignment_size = 128\n",
        "BATCH_SIZE = 32\n",
        "learning_rate = 1e-3\n",
        "MAX_LENGTH = 10\n",
        "alignment_modes = ['dot_product', 'concat', 'general']\n",
        "\n",
        "# Download if not exists\n",
        "if not os.path.exists('fra.txt'):\n",
        "    os.system('wget -q https://www.manythings.org/anki/fra-eng.zip')\n",
        "    os.system('unzip -oq fra-eng.zip')\n",
        "\n",
        "# Load and preprocess\n",
        "text_pairs = []\n",
        "for line in open('fra.txt', 'r'):\n",
        "    a = line.find('CC-BY')\n",
        "    line = line[:a].strip()\n",
        "    if '\\t' not in line:\n",
        "        continue\n",
        "    eng, fra = line.split('\\t')\n",
        "    text_pairs.append((normalizeString(eng), normalizeString(fra)))\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', text_pairs)\n",
        "dataset = DatasetEngFra(pairs, input_lang, output_lang)\n",
        "train_dl = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "ZrV_L7dyr-Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training Loop Across Modes:\n",
        "## Training and Evaluation\n",
        "\n",
        "We now train two models using the Bahdanau attention mechanism with different alignment modes:\n",
        "- **Concat attention** (classic Bahdanau formulation)\n",
        "- **Additive attention** (optimized MLP-based scoring)\n",
        "\n",
        "For each model, we track:\n",
        "- Training loss\n",
        "- Sample translations\n",
        "- Attention visualizations\n"
      ],
      "metadata": {
        "id": "Ad0i0r9KsCBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for alignment_mode in alignment_modes:\n",
        "    print(f\"\\n{'=' * 40} {alignment_mode.upper()} MODE {'=' * 40}\")\n",
        "\n",
        "    encoder = EncoderWithBahdanauAttention(input_lang.n_words, hidden_size).to(device)\n",
        "    decoder = DecoderWithBahdanauAttention(output_lang.n_words, hidden_size, alignment_size=alignment_size,\n",
        "                                           alignment_mode=alignment_mode).to(device)\n",
        "\n",
        "    loss_fn = nn.NLLLoss(ignore_index=PAD_token)\n",
        "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_loss = train_bahdanau_attention(encoder, decoder, train_dl, num_epochs,loss_fn,\n",
        "                                          encoder_optimizer, decoder_optimizer)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for _ in range(10):\n",
        "        eng, fra = random.choice(text_pairs)\n",
        "        print(\"Input:\", eng)\n",
        "        print(\"Target:\", fra)\n",
        "        pred, attentions = translate_with_attention(encoder, decoder, eng, input_lang, output_lang)\n",
        "        print(\"Predicted:\", pred)\n",
        "        print(\"#\" * 80)\n",
        "\n",
        "    # Visualize attention on one sample\n",
        "    sample_input = \"Life is often compared to a journey\"\n",
        "    correct_translation = \"La vie est souvent comparée à un voyage.\"\n",
        "    output_sentence, attention_weights = translate_with_attention(encoder, decoder, sample_input, input_lang, output_lang)\n",
        "    print(\"\\nPredicted translation:\", output_sentence)\n",
        "    print(\"Correct translation:\", correct_translation)\n",
        "    plot_attention(sample_input, output_sentence, attention_weights)\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.plot(train_loss)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f\"Training Loss - {alignment_mode.upper()} Attention\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qCGNMO5xBAhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook implemented **Bahdanau-style attention** for neural machine translation.\n",
        "\n",
        "### Observations:\n",
        "- **Concat attention** follows Bahdanau's original formulation. It produced fluent translations but trained more slowly.\n",
        "- **Additive attention** (a more parameter-efficient variant) trained faster and generally provided similar or better attention focus.\n",
        "\n",
        "Both models produced coherent translations, especially for shorter sequences.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaways\n",
        "- Bahdanau attention helps the decoder focus on relevant encoder states *before* generating the next word.\n",
        "- **Alignment models** (concat/additive) influence convergence speed and context representation.\n",
        "- Attention visualizations provide powerful insights into model behavior.\n",
        "\n"
      ],
      "metadata": {
        "id": "gYgOR0dQBeJs"
      }
    }
  ]
}
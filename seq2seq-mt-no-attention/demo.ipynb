{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Machine Translation is one of the core tasks in Natural Language Processing (NLP), where the goal is to automatically translate a sentence from one language to another. This notebook demonstrates a fundamental implementation of a **Sequence-to-Sequence (Seq2Seq)** model using PyTorch — translating **English to French** without any attention mechanism.\n",
        "\n",
        "The key components of this project include:\n",
        "\n",
        "- Preprocessing English–French sentence pairs from the Tatoeba project\n",
        "- Building vocabulary and mapping words to indices\n",
        "- Implementing a **bidirectional GRU encoder** and a **GRU decoder**\n",
        "- Using **teacher forcing** during training\n",
        "- Handling variable-length sequences with padding and packing\n",
        "- Applying **gradient clipping** to stabilize training\n",
        "- Performing inference using greedy decoding (no beam search)\n",
        "\n",
        "This notebook serves as a minimal, working baseline for Seq2Seq translation, ideal for learning purposes or benchmarking before introducing more advanced techniques like attention or transformers.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-bsUu3J3AfHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install & Import Dependencies"
      ],
      "metadata": {
        "id": "-uHKug2u-wFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages (if needed)\n",
        "# !pip install torch matplotlib\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import unicodedata\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "RNem7Da6-yQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Preprocessing Utilities"
      ],
      "metadata": {
        "id": "BOLidpuF-1Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Special tokens\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "class Language_Dictionary_Builder:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.n_words = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(\" \"):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ],
      "metadata": {
        "id": "f0yREuZM-5Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "Vx4k3JmG-8dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "if not os.path.exists('fra.txt'):\n",
        "    !wget -q https://www.manythings.org/anki/fra-eng.zip\n",
        "    !unzip -oq fra-eng.zip\n",
        "\n",
        "text_pairs = []\n",
        "for line in open('fra.txt', 'r'):\n",
        "    a = line.find('CC-BY')\n",
        "    line = line[:a].strip()\n",
        "    if '\\t' not in line: continue\n",
        "    eng, fra = line.split('\\t')\n",
        "    text_pairs.append((normalizeString(eng), normalizeString(fra)))\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(\" \")) < MAX_LENGTH and len(p[1].split(\" \")) < MAX_LENGTH\n",
        "\n",
        "def prepareData(lang1, lang2, pairs, reverse=False):\n",
        "    if reverse:\n",
        "        pairs = [tuple(reversed(p)) for p in pairs]\n",
        "        input_lang = Language_Dictionary_Builder(lang2)\n",
        "        output_lang = Language_Dictionary_Builder(lang1)\n",
        "    else:\n",
        "        input_lang = Language_Dictionary_Builder(lang1)\n",
        "        output_lang = Language_Dictionary_Builder(lang2)\n",
        "\n",
        "    pairs = [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', text_pairs)\n"
      ],
      "metadata": {
        "id": "Z1iEiGJL-7uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Dataset + Dataloader"
      ],
      "metadata": {
        "id": "FSpZQ6dY_DhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetEngFra(Dataset):\n",
        "    def __init__(self, data, input_lang, output_lang):\n",
        "        self.data = data\n",
        "        self.input_lang = input_lang\n",
        "        self.output_lang = output_lang\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng, fra = self.data[idx]\n",
        "        eng_idx = [self.input_lang.word2index[word] for word in eng.split()]\n",
        "        fra_idx = [self.output_lang.word2index[word] for word in fra.split()]\n",
        "        fra_idx = [SOS_token] + fra_idx + [EOS_token]\n",
        "        return torch.tensor(eng_idx), torch.tensor(fra_idx)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    eng_batch, fra_input, fra_target, eng_len, fra_len = [], [], [], [], []\n",
        "    for eng, fra in batch:\n",
        "        eng_batch.append(eng)\n",
        "        eng_len.append(len(eng))\n",
        "\n",
        "        fra_input.append(fra[:-1])\n",
        "        fra_len.append(len(fra) - 1)\n",
        "        fra_target.append(fra[1:])\n",
        "\n",
        "    eng_pad = nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=PAD_token)\n",
        "    fra_input_pad = nn.utils.rnn.pad_sequence(fra_input, batch_first=True, padding_value=PAD_token)\n",
        "    fra_target_pad = nn.utils.rnn.pad_sequence(fra_target, batch_first=True, padding_value=PAD_token)\n",
        "\n",
        "    return eng_pad, fra_input_pad, fra_target_pad, torch.tensor(eng_len), torch.tensor(fra_len)\n",
        "\n",
        "dataset = DatasetEngFra(pairs, input_lang, output_lang)\n",
        "train_dl = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "pDm3UJSm_IU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Define Encoder & Decoder"
      ],
      "metadata": {
        "id": "cZqorFge_K8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=PAD_token)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        embedded = self.embedding(x)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        outputs, hidden = self.gru(packed)\n",
        "        hidden = hidden[0:hidden.size(0):2] + hidden[1:hidden.size(0):2]\n",
        "        return outputs, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_token)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, hidden, input_seq, lengths):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        output, hidden = self.gru(packed, hidden)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
        "        return nn.functional.log_softmax(self.out(output), dim=-1), hidden\n",
        "\n",
        "    def decode_step(self, input_token, hidden):\n",
        "        embedded = self.embedding(input_token)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        return nn.functional.softmax(self.out(output), dim=-1), hidden\n"
      ],
      "metadata": {
        "id": "0bnmhVS__ONI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Training Function"
      ],
      "metadata": {
        "id": "BNELfEiL_QzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_norm(model):\n",
        "    return sum((p.grad.data.norm(2)**2 for p in model.parameters() if p.grad is not None))**0.5\n",
        "\n",
        "def train_no_attention(encoder, decoder, train_dl, num_epochs, loss_fn, encoder_opt, decoder_opt, device, clip_grad=True, max_norm=1.0):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    all_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for eng, fra_in, fra_tgt, eng_len, fra_len in train_dl:\n",
        "            eng, fra_in, fra_tgt = eng.to(device), fra_in.to(device), fra_tgt.to(device)\n",
        "            eng_len, fra_len = eng_len.to(device), fra_len.to(device)\n",
        "\n",
        "            _, enc_hidden = encoder(eng, eng_len)\n",
        "            output, _ = decoder(enc_hidden, fra_in, fra_len)\n",
        "            loss = loss_fn(output.reshape(-1, output.size(-1)), fra_tgt.reshape(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            if clip_grad:\n",
        "                nn.utils.clip_grad_norm_(encoder.parameters(), max_norm)\n",
        "                nn.utils.clip_grad_norm_(decoder.parameters(), max_norm)\n",
        "\n",
        "            encoder_opt.step(); decoder_opt.step()\n",
        "            encoder_opt.zero_grad(); decoder_opt.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item() * eng.size(0)\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_dl.dataset)\n",
        "        all_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return all_losses\n"
      ],
      "metadata": {
        "id": "liXEX661_Wqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Translate Sentences"
      ],
      "metadata": {
        "id": "e-xdeuQf_Uuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(encoder, decoder, sentence, input_lang, output_lang, device, max_len=MAX_LENGTH):\n",
        "    encoder.eval(); decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        idxs = [input_lang.word2index.get(w, 0) for w in sentence.split()]\n",
        "        input_tensor = torch.tensor(idxs).unsqueeze(0).to(device)\n",
        "        _, hidden = encoder(input_tensor, torch.tensor([len(idxs)]).to(device))\n",
        "\n",
        "        next_token = torch.tensor([[SOS_token]], device=device)\n",
        "        output_words = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            pred, hidden = decoder.decode_step(next_token, hidden)\n",
        "            next_token = torch.argmax(pred, dim=-1)\n",
        "            if next_token.item() == EOS_token:\n",
        "                break\n",
        "            output_words.append(output_lang.index2word.get(next_token.item(), \"<UNK>\"))\n",
        "\n",
        "    return ' '.join(output_words)\n"
      ],
      "metadata": {
        "id": "vqATqK4T_drC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Train the Model"
      ],
      "metadata": {
        "id": "REvmJYGN_iKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "encoder = EncoderRNN(input_lang.n_words, 128).to(device)\n",
        "decoder = DecoderRNN(128, output_lang.n_words).to(device)\n",
        "\n",
        "loss_fn = nn.NLLLoss(ignore_index=PAD_token)\n",
        "encoder_opt = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
        "decoder_opt = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
        "\n",
        "train_loss = train_no_attention(encoder, decoder, train_dl, num_epochs=40, loss_fn=loss_fn,\n",
        "                                encoder_opt=encoder_opt, decoder_opt=decoder_opt, device=device)\n"
      ],
      "metadata": {
        "id": "XIjP0Af8_j3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 9: Plot Loss"
      ],
      "metadata": {
        "id": "H28GJVLQ_qO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HHKTUnLs_sZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Sample Translations"
      ],
      "metadata": {
        "id": "Kx3ulPKM_yTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    eng, fra = random.choice(pairs)\n",
        "    print(f\"Input: {eng}\")\n",
        "    print(f\"Target: {fra}\")\n",
        "    print(f\"Predicted: {translate(encoder, decoder, eng, input_lang, output_lang, device)}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "-2aU052I_z4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion & Next Steps\n",
        "\n",
        "In this notebook, we successfully implemented a complete **English-to-French sequence-to-sequence model** without using attention mechanisms. Despite the absence of attention, the model was able to learn reasonable translations for short and well-structured sentences.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- The **encoder-decoder** architecture is effective for sequence modeling tasks such as translation.\n",
        "- Using **GRUs** (Gated Recurrent Units) helps manage long-range dependencies in sequences.\n",
        "- Proper **padding**, **batching**, and **packed sequences** allow training on variable-length inputs efficiently.\n",
        "- **Teacher forcing** speeds up training by providing the correct target sequence during training.\n",
        "- **Gradient clipping** prevents exploding gradients, especially in RNN-based models.\n",
        "\n",
        "---\n",
        "\n",
        "## Performance Insights\n",
        "\n",
        "- The model works best for **short sentences** (under ~10 tokens) that closely match the training patterns.\n",
        "- Generalization is limited due to the lack of attention and beam search during inference.\n",
        "- Evaluation was done qualitatively; metrics like **BLEU score** could provide more rigorous assessments.\n",
        "\n",
        "---\n",
        "\n",
        "## Future Improvements\n",
        "\n",
        "Here are some ways to improve or extend this project:\n",
        "\n",
        "1. **Add Attention Mechanism**\n",
        "   - Use Luong or Bahdanau-style attention to allow dynamic focus on input tokens during decoding.\n",
        "\n",
        "2. **Use Beam Search for Inference**\n",
        "   - Improves output quality by exploring multiple decoding paths.\n",
        "\n",
        "3. **Train on Larger and More Diverse Data**\n",
        "   - Use full Tatoeba corpus and increase vocabulary coverage.\n",
        "\n",
        "4. **Pretrained Embeddings**\n",
        "   - Integrate GloVe, FastText, or multilingual embeddings to boost language understanding.\n",
        "\n",
        "5. **Evaluation Metrics**\n",
        "   - Add BLEU score, token accuracy, or sequence accuracy to quantitatively assess performance.\n",
        "\n",
        "6. **Checkpointing**\n",
        "   - Add support for saving and loading trained models.\n",
        "\n",
        "---\n",
        "\n",
        "This project serves as a solid foundation for anyone looking to dive into neural machine translation. With a clear structure and modular design, it's ready to be extended into more powerful architectures like attention-based models or transformers.\n",
        "\n",
        "Happy translating!\n"
      ],
      "metadata": {
        "id": "e0BUBYJmA4o-"
      }
    }
  ]
}